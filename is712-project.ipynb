{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "is712-project.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wO7qBjEbwrSh",
        "colab": {}
      },
      "source": [
        "print('Hello World!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXLqpEo3c3eh",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19dJxAvrbj47",
        "colab_type": "code",
        "outputId": "3308255f-af3f-45ea-a44e-22edc7f6c35d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_lQDyb4d-XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq /content/drive/My\\ Drive/IS712/data/train.zip -d ./data\n",
        "!unzip -qq /content/drive/My\\ Drive/IS712/data/public_test.zip -d ./data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X5B785nez_L",
        "colab_type": "code",
        "outputId": "0ab5ea9f-7ad0-4b33-df43-79a04a58438c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct 17 12:49:56 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXi5R5Jji7zc",
        "colab_type": "text"
      },
      "source": [
        "# Global Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ8jyBm5jBR_",
        "colab_type": "code",
        "outputId": "0e8f701e-e174-4be3-b415-6ffa25bef0f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from datetime import datetime\n",
        "from itertools import combinations\n",
        "from itertools import permutations\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tnrange\n",
        "import platform\n",
        "print(platform.python_version())\n",
        "\n",
        "# clear the computational graph for evey new trial #\n",
        "tf.reset_default_graph()\n",
        "# ##\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "random.seed(712)\n",
        "np.random.seed(712)\n",
        "\n",
        "print(\"Numpy version:\", np.__version__)\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.8\n",
            "Numpy version: 1.16.5\n",
            "TensorFlow version: 1.15.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7oS5-spjGBw",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lx2rO98hUfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"./data/train\"\n",
        "TEST_SIZE = 0.2\n",
        "# IMG_SIZE = 32\n",
        "IMG_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUiw7G51hsXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### DATA SAMPLING ###\n",
        "\n",
        "def sample(folder_img_paths):\n",
        "    data = []\n",
        "    for folder, img_paths in folder_img_paths.items():\n",
        "        # positive pairs\n",
        "        # pos_pairs = [(p1, p2) for (p1, p2) in combinations(img_paths, 2)]\n",
        "        pos_pairs = [(p1, p2) for (p1, p2) in permutations(img_paths, 2)]\n",
        "\n",
        "        # negative pairs\n",
        "        neg_pairs = []\n",
        "        for _ in range(len(pos_pairs)):\n",
        "            img_path1 = np.random.choice(img_paths, 1)[0]\n",
        "            # sample negative folder\n",
        "            neg_folders = [f for f in folder_img_paths.keys() if f != folder]\n",
        "            assert len(neg_folders) == len(folder_img_paths.keys()) - 1\n",
        "            neg_folder = np.random.choice(neg_folders, 1)[0]\n",
        "            # sample negative image\n",
        "            img_path2 = np.random.choice(folder_img_paths[neg_folder], 1)[0]\n",
        "            neg_pairs.append((img_path1, img_path2))\n",
        "            # Jiaqi, swap the sequence of the two images to generate more data #\n",
        "            # neg_pairs.append((img_path2, img_path1))\n",
        "            \n",
        "        # combine positive and negative data\n",
        "        data.extend([(p1, p2, 1) for (p1, p2) in pos_pairs])\n",
        "        data.extend([(p1, p2, 0) for (p1, p2) in neg_pairs])\n",
        "\n",
        "    random.shuffle(data)\n",
        "    return np.asarray(data)\n",
        "\n",
        "\n",
        "def gen_data():\n",
        "    train_img_paths = {}\n",
        "    test_img_paths = {}\n",
        "    for folder in glob.glob(DATA_DIR + '/*/'):\n",
        "        img_paths = [p for p in glob.glob(folder + '/*.jpg')]\n",
        "        random.shuffle(img_paths)\n",
        "        n_train = int(len(img_paths) * (1 - TEST_SIZE))\n",
        "        train_img_paths[folder] = img_paths[:n_train]\n",
        "        test_img_paths[folder] = img_paths[n_train:]\n",
        "\n",
        "    train_data = sample(train_img_paths)\n",
        "    test_data = sample(test_img_paths)\n",
        "    print('Training size: {}'.format(len(train_data)))\n",
        "    print('Test size: {}'.format(len(test_data)))\n",
        "    return train_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t596AiumhyY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### IMAGE READING PARSING ###\n",
        "\n",
        "def read_img(img_path, is_training=False):\n",
        "    img_string = tf.read_file(img_path)\n",
        "    img_decoded = tf.image.decode_jpeg(img_string, channels=3)\n",
        "    img = tf.image.resize(img_decoded, [IMG_SIZE, IMG_SIZE])\n",
        "    img = img / 255.0\n",
        "\n",
        "    if is_training:\n",
        "        \"\"\"Data augmentation comes here\"\"\"\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def parse_function(img_path1, img_path2, label, is_training=False):\n",
        "    img1 = read_img(img_path1, is_training)\n",
        "    img2 = read_img(img_path2, is_training)\n",
        "    return img1, img2, [label]\n",
        "\n",
        "\n",
        "def parse_function_train(img_path1, img_path2, label):\n",
        "    return parse_function(img_path1, img_path2, label, is_training=True)\n",
        "\n",
        "\n",
        "def parse_function_test(img_path1, img_path2, label):\n",
        "    return parse_function(img_path1, img_path2, label, is_training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHM5-XMZh24w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### DATA SERVING ###\n",
        "\n",
        "class DataGenerator(object):\n",
        "\n",
        "    def __init__(self, batch_size=1, num_threads=1, \n",
        "                 train_shuffle=False, buffer_size=10000):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_threads = num_threads\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "        # data sampling and spliting\n",
        "        self.train_data, self.test_data = gen_data()\n",
        "\n",
        "        # build iterator\n",
        "        self.train_set = self._build_data_set(self.train_data, \n",
        "                                              parse_function_train, \n",
        "                                              shuffle=train_shuffle)\n",
        "        self.iterator = tf.data.Iterator.from_structure(self.train_set.output_types,\n",
        "                                                        self.train_set.output_shapes)\n",
        "        # for training\n",
        "        self.train_init_op = self.iterator.make_initializer(self.train_set)\n",
        "        self.next = self.iterator.get_next()\n",
        "        self.num_train_batches = int(np.ceil(len(self.train_data) / batch_size))\n",
        "        # for testing\n",
        "        self.test_set = self._build_data_set(self.test_data, parse_function_test)\n",
        "        self.test_init_op = self.iterator.make_initializer(self.test_set)\n",
        "        self.num_test_batches = int(np.ceil(len(self.test_data) / batch_size))\n",
        "\n",
        "    def _build_data_set(self, data, map_fn, shuffle=False):\n",
        "        \"\"\"\n",
        "        Images are loaded from disk and processed batch by batch. Since our dataset\n",
        "        is not that big, it would be faster if we load all the images into RAM once \n",
        "        and read from their. I leave it for you guys to explore :)\n",
        "        \"\"\"\n",
        "        img_path1 = tf.convert_to_tensor(data[:, 0], dtype=tf.string)\n",
        "        img_path2 = tf.convert_to_tensor(data[:, 1], dtype=tf.string)\n",
        "        labels = tf.convert_to_tensor(data[:, 2], dtype=tf.int32)\n",
        "        data = tf.data.Dataset.from_tensor_slices((img_path1, img_path2, labels))\n",
        "        if shuffle:\n",
        "            data = data.shuffle(buffer_size=self.buffer_size)\n",
        "        data = data.map(map_fn, num_parallel_calls=self.num_threads)\n",
        "        data = data.batch(self.batch_size)\n",
        "        data = data.prefetch(self.num_threads)\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBOFGm76iG9M",
        "colab_type": "text"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzBkEgGviTxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(object):\n",
        "\n",
        "    def __init__(self, training=False):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, IMG_SIZE, IMG_SIZE, 3])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, IMG_SIZE, IMG_SIZE, 3])\n",
        "        self.y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "        net1 = self._encoder(self.x1)\n",
        "        net2 = self._encoder(self.x2)\n",
        "        net = tf.abs(net1 - net2)\n",
        "        # try concatenate #\n",
        "        # net = tf.concat([net1, net2], 1)\n",
        "\n",
        "        with tf.variable_scope('classifier'):\n",
        "            self.logits = tf.layers.dense(net, 1, name='logits')\n",
        "            self.prob = tf.nn.sigmoid(self.logits, name='prob')\n",
        "\n",
        "        if training:\n",
        "            self.loss, self.train_op = self._loss_fn()\n",
        "\n",
        "    def _encoder(self, input, name='encoder', reuse=False):\n",
        "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "              with tf.variable_scope(\"conv1\") as scope:\n",
        "                   net = tf.contrib.layers.conv2d(input, 64, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 64, [3, 3], activation_fn=tf.nn.relu, padding='SAME', weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.max_pool2d(net, [2, 2], stride=2, padding='SAME')\n",
        "              \n",
        "              with tf.variable_scope(\"conv2\") as scope:\n",
        "                   net = tf.contrib.layers.conv2d(net, 128, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 128, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.max_pool2d(net, [2, 2], stride=2, padding='SAME')\n",
        "              \n",
        "              with tf.variable_scope(\"conv3\") as scope:\n",
        "                   net = tf.contrib.layers.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse) \n",
        "                  #net = tf.contrib.layers.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                   # weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   #net = tf.contrib.layers.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                   # weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.max_pool2d(net, [2, 2], stride=2, padding='SAME')\n",
        "          \n",
        "              with tf.variable_scope(\"conv4\") as scope:\n",
        "                   net = tf.contrib.layers.conv2d(net, 512, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 512, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 512, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.max_pool2d(net, [2, 2], stride=2, padding='SAME')\n",
        "              \n",
        "              with tf.variable_scope(\"conv5\") as scope:\n",
        "                   net = tf.contrib.layers.conv2d(net, 512, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 512, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.conv2d(net, 512, [3, 3], activation_fn=tf.nn.relu, padding='SAME', \n",
        "                    weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "                   net = tf.contrib.layers.max_pool2d(net, [2, 2], stride=2, padding='SAME')\n",
        "              \n",
        "\t\t\t             #net = tf.contrib.layers.conv2d(net, 2, [1, 1], activation_fn=None, padding='SAME', \n",
        "                   # weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n",
        "\t\t\t             #net = tf.contrib.layers.max_pool2d(net, [2, 2], padding='SAME')\n",
        "\t\t          \n",
        "        \n",
        "              net = tf.layers.flatten(net)\n",
        "              #net = tf.layers.dense(net, units=300, activation=tf.nn.sigmoid)\n",
        "              net = tf.layers.dense(net, units=1000, activation=tf.nn.relu)\n",
        "              return net\n",
        "\n",
        "    def _loss_fn(self):\n",
        "        trained_vars = tf.trainable_variables()\n",
        "\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, \n",
        "                                                                logits=self.logits)\n",
        "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "        l2_reg = tf.add_n([tf.nn.l2_loss(v) for v in trained_vars \n",
        "                           if 'bias' not in v.name])\n",
        "        loss = cross_entropy + LAMBDA_REG * l2_reg\n",
        "\n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE, \n",
        "                                           beta1=0.9, \n",
        "                                           beta2=0.99, \n",
        "                                           epsilon=1e-8)\n",
        "        train_op = optimizer.minimize(loss, global_step, var_list=trained_vars)\n",
        "\n",
        "        return loss, train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNKkbNLpiYoD",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqnf0BPiovG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "# BATCH_SIZE = 256\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 0.001\n",
        "LAMBDA_REG = 0.0\n",
        "\n",
        "NUM_CHECKPOINTS = 5\n",
        "NUM_THREADS = 4\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/My Drive/IS712/checkpoints\"\n",
        "if tf.gfile.Exists(CHECKPOINT_DIR):\n",
        "    tf.gfile.DeleteRecursively(CHECKPOINT_DIR)\n",
        "tf.gfile.MakeDirs(CHECKPOINT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtrRxcdCjPy3",
        "colab_type": "code",
        "outputId": "3ac02ec3-1b6a-41b9-aa20-816133b0f70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "generator = DataGenerator(batch_size=BATCH_SIZE, num_threads=NUM_THREADS, \n",
        "                          train_shuffle=True, buffer_size=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training size: 40000\n",
            "Test size: 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6eToOvwjVj-",
        "colab_type": "code",
        "outputId": "d63efdb6-3473-40de-e8a9-6081807460c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Just a useful function for parameter counting \n",
        "def count_parameters(trained_vars):\n",
        "    total_parameters = 0\n",
        "    print('=' * 100)\n",
        "    for variable in trained_vars:\n",
        "        variable_parameters = 1\n",
        "        for dim in variable.get_shape():\n",
        "            variable_parameters *= dim.value\n",
        "        print('{:70} {:20} params'.format(variable.name, variable_parameters))\n",
        "        print('-' * 100)\n",
        "        total_parameters += variable_parameters\n",
        "    print('=' * 100)\n",
        "    print(\"Total trainable parameters: %d\" % total_parameters)\n",
        "    print('=' * 100)\n",
        "\n",
        "\n",
        "model = MLP(training=True)\n",
        "count_parameters(tf.trainable_variables())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "encoder/conv1/weights:0                                                                1536 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv1/biases:0                                                                   32 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv2/weights:0                                                               16384 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv2/biases:0                                                                   32 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv3/weights:0                                                               18432 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv3/biases:0                                                                   64 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv4/weights:0                                                               36864 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv4/biases:0                                                                   64 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv5/weights:0                                                                 128 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/conv5/biases:0                                                                    2 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/dense/kernel:0                                                               153600 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "encoder/dense/bias:0                                                                    300 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "classifier/logits/kernel:0                                                              300 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "classifier/logits/bias:0                                                                  1 params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "====================================================================================================\n",
            "Total trainable parameters: 227739\n",
            "====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYQVMx4-jXl7",
        "colab_type": "code",
        "outputId": "3fda22d8-5cb6-4d27-c087-e1359a874463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=NUM_CHECKPOINTS)\n",
        "    \n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        print(\"\\n{} Epoch: {}/{}\".format(datetime.now(), epoch, NUM_EPOCHS))\n",
        "\n",
        "        # Training\n",
        "        sum_loss = 0.\n",
        "        sess.run(generator.train_init_op)\n",
        "        loop = tnrange(generator.num_train_batches, desc='Training')\n",
        "        for step in loop:\n",
        "            batch_img1, batch_img2, batch_label = sess.run(generator.next)\n",
        "            _, loss = sess.run([model.train_op, model.loss], \n",
        "                                feed_dict={model.x1: batch_img1,\n",
        "                                           model.x2: batch_img2,\n",
        "                                           model.y: batch_label})\n",
        "            sum_loss += loss\n",
        "            loop.set_postfix(loss=(sum_loss / (step + 1)))\n",
        "        print('Training loss: {:.6f}'.format(sum_loss))\n",
        "  \n",
        "        saver.save(sess, \n",
        "                   os.path.join(CHECKPOINT_DIR, 'model_e{}.ckpt'.format(epoch)))\n",
        "          \n",
        "        # Testing\n",
        "        pds = []\n",
        "        gts = []\n",
        "        sum_loss = 0.\n",
        "        sess.run(generator.test_init_op)\n",
        "        loop = tnrange(generator.num_test_batches, desc='Testing')\n",
        "        for step in loop:\n",
        "            batch_img1, batch_img2, batch_label = sess.run(generator.next)\n",
        "            prob, loss = sess.run([model.prob, model.loss],\n",
        "                                   feed_dict={model.x1: batch_img1,\n",
        "                                              model.x2: batch_img2,\n",
        "                                              model.y: batch_label})\n",
        "            sum_loss += loss\n",
        "            loop.set_postfix(loss=(sum_loss / (step + 1)))\n",
        "            pds.extend(np.round(prob).ravel().tolist())\n",
        "            gts.extend(batch_label.ravel().tolist())\n",
        "        pds = np.asarray(pds)\n",
        "        gts = np.asarray(gts)\n",
        "        print('Test loss: {:.6f}'.format(sum_loss))\n",
        "        print('Test acc: {:.6f}'.format(np.equal(pds, gts).sum() / len(gts)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2019-10-17 13:34:03.175762 Epoch: 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "432d5cbedd05409c8315d93242dd7ab7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=313, style=ProgressStyle(description_width='in…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5479\u001b[0m                  self).get_controller(default) as g, context.graph_mode():\n\u001b[0;32m-> 5480\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5481\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-118c94febc7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mbatch_img1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_img2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             _, loss = sess.run([model.train_op, model.loss], \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-118c94febc7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss: {:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test acc: {:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exec_type, exec_value, exec_tb)\u001b[0m\n\u001b[1;32m   1632\u001b[0m       \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m       \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m         logging.error(\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6TrCqEvllDt",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwGWGFQBlo4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"./data/public_test\"\n",
        "CHECKPOINT = \"/content/drive/My Drive/IS712/checkpoints/model_e{}.ckpt\".format(NUM_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVJswtUlsUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_test_data(input_file):\n",
        "    test_data = []\n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f:\n",
        "            img1, img2 = line.strip().split('\\t')\n",
        "            test_data.append((os.path.join(DATA_DIR, 'images', img1),\n",
        "                              os.path.join(DATA_DIR, 'images', img2)))\n",
        "    return np.asarray(test_data)\n",
        "\n",
        "\n",
        "def data_generator(input_file):\n",
        "    test_data = read_test_data(input_file)\n",
        "    img_path1 = tf.convert_to_tensor(test_data[:, 0], dtype=tf.string)\n",
        "    img_path2 = tf.convert_to_tensor(test_data[:, 1], dtype=tf.string)\n",
        "\n",
        "    def parse_function(img_path1, img_path2):\n",
        "        img1 = read_img(img_path1, is_training=False)\n",
        "        img2 = read_img(img_path2, is_training=False)\n",
        "        return img1, img2\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((img_path1, img_path2))\n",
        "    test_set = test_set.map(parse_function, num_parallel_calls=NUM_THREADS)\n",
        "    test_set = test_set.batch(BATCH_SIZE)\n",
        "    test_set = test_set.prefetch(NUM_THREADS)\n",
        "\n",
        "    iterator = tf.data.Iterator.from_structure(test_set.output_types, test_set.output_shapes)\n",
        "    init_op = iterator.make_initializer(test_set)\n",
        "    next = iterator.get_next()\n",
        "    num_batches = int(np.ceil(len(test_data) / BATCH_SIZE))\n",
        "\n",
        "    return init_op, next, num_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHBX090clx14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('./submission'):\n",
        "    os.makedirs('./submission')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "model = MLP(training=False)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    tf.train.Saver().restore(sess, CHECKPOINT)\n",
        "\n",
        "    for input_file in glob.glob(os.path.join(DATA_DIR, 'pairs/*.txt')):\n",
        "        basename = os.path.basename(input_file)\n",
        "        init_op, next, num_batches = data_generator(input_file)\n",
        "        pds = []\n",
        "        sess.run(init_op)\n",
        "        for _ in tnrange(num_batches, desc=basename):\n",
        "            batch_img1, batch_img2 = sess.run(next)\n",
        "            prob = sess.run(model.prob, feed_dict={model.x1: batch_img1,\n",
        "                                                   model.x2: batch_img2})\n",
        "            pds.extend(np.round(prob).ravel().tolist())\n",
        "\n",
        "        # write prediction to submission files\n",
        "        with open('./submission/{}'.format(basename), 'w') as f:\n",
        "            f.write('\\n'.join(str(int(pd)) for pd in pds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5poEjWll0gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd submission; zip /content/drive/My\\ Drive/IS712/submission.zip *"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}